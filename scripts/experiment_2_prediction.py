#!/usr/bin/env python3
"""
Synapse Experiment 2: Next Action Prediction Accuracy

- Implement baseline models (frequency model, Markov model)
- Train a simple LSTM sequence model
- Compare different models' next action prediction accuracy
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter, defaultdict
from scipy import stats
from scipy.fftpack import dct
import argparse
import os
import matplotlib.pyplot as plt
import seaborn as sns

# Optional deep learning support
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import (Embedding, LSTM, GRU, Dense, Dropout, 
                                       MultiHeadAttention, LayerNormalization, 
                                       GlobalAveragePooling1D, Input)
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.callbacks import EarlyStopping
    HAS_TENSORFLOW = True
except ImportError:
    print("Warning: TensorFlow not found. LSTM, GRU, and Transformer models will be skipped.")
    print("Install command: pip install tensorflow")
    HAS_TENSORFLOW = False

class PredictionExperiment:
    def __init__(self, cleaned_data_file: str):
        self.df = pd.read_csv(cleaned_data_file)
        self.results = {}
        # å­˜å‚¨æ¶ˆèç ”ç©¶çš„ç»“æœ
        self.ablation_results = {}
        self.prepare_data()

    def prepare_data(self):
        """Convert event sequences to model-ready format"""
        print("Preparing data...")
        
        # Clean data
        self.df = self.df.dropna(subset=['action_subtype'])
        
        # Create simplified token sequence
        self.df['token'] = self.df['action_subtype'].astype(str)
        
        # Add more context information to tokens
        enhanced_tokens = []
        for _, row in self.df.iterrows():
            token = row['action_subtype']
            
            # Add element role information for click events
            if token == 'click' and pd.notna(row['element_role']):
                token = f"click_{row['element_role']}"
            
            # Add modifier key information for keyboard events
            elif token == 'keydown':
                if row['is_ctrl_key']:
                    token = "ctrl_key"
                elif row['is_shift_key']:
                    token = "shift_key"
                elif row['is_alt_key']:
                    token = "alt_key"
                else:
                    token = "regular_key"
            
            enhanced_tokens.append(token)
        
        self.df['enhanced_token'] = enhanced_tokens
        
        # Create sequence data
        self.create_sequences()
        
        print(f"Data preparation completed:")
        print(f"- Total events: {len(self.df)}")
        print(f"- Unique tokens: {len(self.df['enhanced_token'].unique())}")
        print(f"- Training sequences: {len(self.X_train) if hasattr(self, 'X_train') else 0}")

    def extract_webfast_features(self, sequence):
        """
        æå–WebFASTç‰¹å¾ï¼ˆæ¨¡æ‹Ÿml-worker.tsä¸­çš„é€»è¾‘ï¼‰
        ä½¿ç”¨DCTå˜æ¢æå–æ—¶åºç‰¹å¾
        """
        features = []
        
        # æ—¶é—´ç‰¹å¾ - ä½¿ç”¨DCTå˜æ¢
        if len(sequence) > 1:
            timestamps = [event.timestamp for event in sequence if hasattr(event, 'timestamp')]
            if len(timestamps) > 1:
                time_diffs = np.diff(timestamps)
                if len(time_diffs) >= 3:
                    # å¯¹æ—¶é—´é—´éš”åºåˆ—è¿›è¡ŒDCTå˜æ¢
                    time_dct = dct(time_diffs[:min(10, len(time_diffs))], type=2, norm='ortho')
                    features.extend(time_dct[:5])  # å–å‰5ä¸ªDCTç³»æ•°
                else:
                    features.extend([0] * 5)
            else:
                features.extend([0] * 5)
        else:
            features.extend([0] * 5)
        
        # äº‹ä»¶ç±»å‹ç‰¹å¾ï¼ˆä½¿ç”¨hashç¼–ç ï¼‰
        type_hashes = []
        for event in sequence:
            event_type = getattr(event, 'enhanced_token', getattr(event, 'token', 'unknown'))
            type_hash = hash(event_type) % 100  # æ¨¡æ‹Ÿml-worker.tsä¸­çš„hashString
            type_hashes.append(type_hash)
        
        if type_hashes:
            # å¯¹ç±»å‹åºåˆ—ä¹Ÿè¿›è¡ŒDCTå˜æ¢
            if len(type_hashes) >= 3:
                type_dct = dct(type_hashes[:min(10, len(type_hashes))], type=2, norm='ortho')
                features.extend(type_dct[:5])
            else:
                features.extend(type_hashes + [0] * (5 - len(type_hashes)))
        else:
            features.extend([0] * 5)
        
        # åºåˆ—ç»Ÿè®¡ç‰¹å¾
        features.extend([
            len(sequence),  # åºåˆ—é•¿åº¦
            np.var(type_hashes) if type_hashes else 0,  # ç±»å‹æ–¹å·®
            len(set(type_hashes)) if type_hashes else 0,  # å”¯ä¸€ç±»å‹æ•°
        ])
        
        # æ ‡å‡†åŒ–åˆ°å›ºå®šé•¿åº¦
        target_length = 13
        if len(features) < target_length:
            features.extend([0] * (target_length - len(features)))
        
        return np.array(features[:target_length])
    
    def extract_baseline_features(self, sequence):
        """
        æå–åŸºçº¿ç‰¹å¾ï¼ˆç®€å•çš„ç»Ÿè®¡ç‰¹å¾ï¼Œä¸ä½¿ç”¨DCTï¼‰
        """
        features = []
        
        # ç®€å•çš„ç»Ÿè®¡ç‰¹å¾
        features.extend([
            len(sequence),  # åºåˆ—é•¿åº¦
        ])
        
        # äº‹ä»¶ç±»å‹çš„one-hotç¼–ç ï¼ˆç®€åŒ–ç‰ˆï¼‰
        common_types = ['click', 'keydown', 'text_input', 'scroll', 'focus_change']
        for event_type in common_types:
            count = sum(1 for event in sequence 
                       if event_type in getattr(event, 'enhanced_token', 
                                               getattr(event, 'token', 'unknown')))
            features.append(count / len(sequence))  # å½’ä¸€åŒ–é¢‘ç‡
        
        # åºåˆ—çš„ç®€å•ç»Ÿè®¡
        type_tokens = [getattr(event, 'enhanced_token', 
                              getattr(event, 'token', 'unknown')) for event in sequence]
        
        features.extend([
            len(set(type_tokens)),  # å”¯ä¸€ç±»å‹æ•°
            type_tokens.count('click') / len(sequence) if sequence else 0,  # ç‚¹å‡»æ¯”ä¾‹
            type_tokens.count('keydown') / len(sequence) if sequence else 0,  # æŒ‰é”®æ¯”ä¾‹
        ])
        
        # æ ‡å‡†åŒ–åˆ°å›ºå®šé•¿åº¦
        target_length = 13
        if len(features) < target_length:
            features.extend([0] * (target_length - len(features)))
        
        return np.array(features[:target_length])
    
    def create_sequences(self):
        """Create input-output sequence pairs"""
        tokens = self.df['enhanced_token'].tolist()
        
        # Create vocabulary
        self.vocab = list(set(tokens))
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for token, i in self.token_to_id.items()}
        
        # Convert to numeric sequence
        token_ids = [self.token_to_id[token] for token in tokens]
        
        # Create sliding window sequences
        seq_length = 5  # Use previous 5 events to predict the 6th
        X, y = [], []
        
        for i in range(len(token_ids) - seq_length):
            X.append(token_ids[i:i+seq_length])
            y.append(token_ids[i+seq_length])
        
        if len(X) == 0:
            print("Error: Too little data to create sequences")
            return
        
        X = np.array(X)
        y = np.array(y)
        
        # Split training and test sets
        if len(X) < 10:
            print("Warning: Very small dataset, results may be unreliable")
            test_size = 0.3
        else:
            test_size = 0.2
            
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, shuffle=False
        )
    
    def evaluate_enhanced_metrics(self, model_name: str, y_pred: list, y_pred_proba: list = None):
        """
        è¯„ä¼°å¢å¼ºæŒ‡æ ‡: Top-Kå‡†ç¡®ç‡ã€æ–°é¢–æ€§ä¸å¤šæ ·æ€§ã€è¦†ç›–ç‡
        æ ¹æ®CLAUDE.mdè¦æ±‚å®ç°
        """
        metrics = {}
        
        # 1. Top-Kå‡†ç¡®ç‡ (K=3, K=5)
        if y_pred_proba is not None:
            # å¯¹äºæ¦‚ç‡é¢„æµ‹ï¼Œè®¡ç®—Top-K
            for k in [3, 5]:
                top_k_acc = self.calculate_top_k_accuracy(y_pred_proba, self.y_test, k)
                metrics[f'top_{k}_accuracy'] = top_k_acc
        else:
            # å¯¹äºç¡®å®šæ€§é¢„æµ‹ï¼Œä½¿ç”¨è¿‘ä¼¼æ–¹æ³•
            metrics['top_3_accuracy'] = metrics.get('top_3_accuracy', accuracy_score(self.y_test, y_pred))
            metrics['top_5_accuracy'] = metrics.get('top_5_accuracy', accuracy_score(self.y_test, y_pred))
        
        # 2. æ–°é¢–æ€§ä¸å¤šæ ·æ€§ (Novelty & Diversity)
        novelty_score = self.calculate_novelty(y_pred, self.y_train)
        diversity_score = self.calculate_diversity(y_pred)
        metrics['novelty'] = novelty_score
        metrics['diversity'] = diversity_score
        
        # 3. è¦†ç›–ç‡ (Coverage) - æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé¢„æµ‹çš„åœºæ™¯æ¯”ä¾‹
        coverage_score = self.calculate_coverage(y_pred)
        metrics['coverage'] = coverage_score
        
        # 4. é¢„æµ‹åˆ†å¸ƒçš„ç†µ (é¢„æµ‹ä¸ç¡®å®šæ€§)
        prediction_entropy = self.calculate_prediction_entropy(y_pred)
        metrics['prediction_entropy'] = prediction_entropy
        
        return metrics
    
    def calculate_top_k_accuracy(self, y_pred_proba, y_true: list, k: int) -> float:
        """è®¡ç®—Top-Kå‡†ç¡®ç‡"""
        if y_pred_proba is None or len(y_pred_proba) == 0:
            return 0.0
        
        correct = 0
        total = len(y_true)
        
        for i, true_label in enumerate(y_true):
            if i < len(y_pred_proba):
                # è·å–å‰Kä¸ªæœ€å¯èƒ½çš„é¢„æµ‹
                if isinstance(y_pred_proba[i], dict):
                    # å¦‚æœæ˜¯æ¦‚ç‡å­—å…¸æ ¼å¼
                    top_k_preds = sorted(y_pred_proba[i].items(), key=lambda x: x[1], reverse=True)[:k]
                    top_k_labels = [pred[0] for pred in top_k_preds]
                elif isinstance(y_pred_proba[i], (list, np.ndarray)):
                    # å¦‚æœæ˜¯æ¦‚ç‡æ•°ç»„æ ¼å¼
                    top_k_indices = np.argsort(y_pred_proba[i])[-k:][::-1]
                    top_k_labels = top_k_indices
                else:
                    continue
                    
                if true_label in top_k_labels:
                    correct += 1
        
        return correct / total if total > 0 else 0.0
    
    def calculate_novelty(self, y_pred: list, y_train: list) -> float:
        """
        è®¡ç®—æ–°é¢–æ€§: é¢„æµ‹ä¸­åŒ…å«å¤šå°‘è®­ç»ƒé›†ä¸­ç½•è§çš„åŠ¨ä½œ
        é«˜æ–°é¢–æ€§æ„å‘³ç€æ¨¡å‹ä¸åªæ˜¯é¢„æµ‹å¸¸è§åŠ¨ä½œ
        """
        # è®¡ç®—è®­ç»ƒé›†ä¸­æ¯ä¸ªåŠ¨ä½œçš„é¢‘ç‡
        train_counts = Counter(y_train)
        total_train = len(y_train)
        
        # è®¡ç®—é¢„æµ‹çš„æ–°é¢–æ€§åˆ†æ•°
        novelty_scores = []
        for pred in y_pred:
            # ä½¿ç”¨é€†é¢‘ç‡ä½œä¸ºæ–°é¢–æ€§åº¦é‡
            frequency = train_counts.get(pred, 0) / total_train
            novelty = 1.0 / (frequency + 1e-6)  # æ·»åŠ å°å¸¸æ•°é¿å…é™¤é›¶
            novelty_scores.append(novelty)
        
        return np.mean(novelty_scores)
    
    def calculate_diversity(self, y_pred: list) -> float:
        """
        è®¡ç®—å¤šæ ·æ€§: é¢„æµ‹ç»“æœçš„å¤šæ ·æ€§ç¨‹åº¦
        ä½¿ç”¨Shannonç†µæ¥è¡¡é‡é¢„æµ‹åˆ†å¸ƒçš„å¤šæ ·æ€§
        """
        if len(y_pred) == 0:
            return 0.0
            
        # è®¡ç®—é¢„æµ‹åˆ†å¸ƒ
        pred_counts = Counter(y_pred)
        pred_probs = [count / len(y_pred) for count in pred_counts.values()]
        
        # è®¡ç®—Shannonç†µ
        diversity = -sum(p * np.log2(p) for p in pred_probs if p > 0)
        
        # æ ‡å‡†åŒ–åˆ°[0,1]åŒºé—´
        max_diversity = np.log2(len(pred_counts))
        normalized_diversity = diversity / max_diversity if max_diversity > 0 else 0.0
        
        return normalized_diversity
    
    def calculate_coverage(self, y_pred: list) -> float:
        """
        è®¡ç®—è¦†ç›–ç‡: æ¨¡å‹é¢„æµ‹è¦†ç›–äº†å¤šå°‘ç§ä¸åŒçš„åŠ¨ä½œç±»å‹
        """
        unique_predictions = len(set(y_pred))
        total_possible_actions = len(self.vocab)
        
        coverage = unique_predictions / total_possible_actions
        return coverage
    
    def calculate_prediction_entropy(self, y_pred: list) -> float:
        """è®¡ç®—é¢„æµ‹åˆ†å¸ƒçš„ç†µï¼Œè¡¡é‡é¢„æµ‹çš„ä¸ç¡®å®šæ€§"""
        if len(y_pred) == 0:
            return 0.0
            
        pred_counts = Counter(y_pred)
        pred_probs = [count / len(y_pred) for count in pred_counts.values()]
        
        entropy = -sum(p * np.log2(p) for p in pred_probs if p > 0)
        return entropy

    def run_frequency_baseline(self):
        """Run frequency baseline model"""
        print("\n--- Frequency Baseline Model ---")
        
        # Find the most common token
        most_common_id = Counter(self.y_train).most_common(1)[0][0]
        most_common_token = self.id_to_token[most_common_id]
        
        # Predict the most common token for all test samples
        y_pred_freq = [most_common_id] * len(self.y_test)
        
        accuracy = accuracy_score(self.y_test, y_pred_freq)
        
        # è®¡ç®—å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        enhanced_metrics = self.evaluate_enhanced_metrics('frequency', y_pred_freq)
        
        self.results['frequency'] = {
            'accuracy': accuracy,
            'model': f'æ€»æ˜¯é¢„æµ‹: {most_common_token}',
            **enhanced_metrics
        }
        
        print(f"Most common action: {most_common_token}")
        print(f"Top-1 accuracy: {accuracy:.3f}")
        print(f"Top-3 accuracy: {enhanced_metrics['top_3_accuracy']:.3f}")
        print(f"Top-5 accuracy: {enhanced_metrics['top_5_accuracy']:.3f}")
        print(f"Novelty score: {enhanced_metrics['novelty']:.3f}")
        print(f"Diversity score: {enhanced_metrics['diversity']:.3f}")
        print(f"Coverage: {enhanced_metrics['coverage']:.3f}")
        
        return accuracy

    def run_markov_baseline(self):
        """Run Markov baseline model"""
        print("\n--- Markov Baseline Model ---")
        
        # Build transition probability matrix
        transitions = defaultdict(Counter)
        
        for i in range(len(self.y_train)):
            prev_token = self.X_train[i][-1]  # ä½¿ç”¨åºåˆ—çš„æœ€åä¸€ä¸ªtoken
            next_token = self.y_train[i]
            transitions[prev_token][next_token] += 1
        
        # Predict test set
        y_pred_markov = []
        fallback_token = Counter(self.y_train).most_common(1)[0][0]
        
        for seq in self.X_test:
            prev_token = seq[-1]
            
            if prev_token in transitions and transitions[prev_token]:
                # Select the most likely next token
                pred = transitions[prev_token].most_common(1)[0][0]
                y_pred_markov.append(pred)
            else:
                # Fall back to most common token
                y_pred_markov.append(fallback_token)
        
        accuracy = accuracy_score(self.y_test, y_pred_markov)
        
        # è®¡ç®—å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        enhanced_metrics = self.evaluate_enhanced_metrics('markov', y_pred_markov)
        
        self.results['markov'] = {
            'accuracy': accuracy,
            'transitions': len(transitions),
            **enhanced_metrics
        }
        
        print(f"Learned transition patterns: {len(transitions)}")
        print(f"Top-1 accuracy: {accuracy:.3f}")
        print(f"Top-3 accuracy: {enhanced_metrics['top_3_accuracy']:.3f}")
        print(f"Top-5 accuracy: {enhanced_metrics['top_5_accuracy']:.3f}")
        print(f"Novelty score: {enhanced_metrics['novelty']:.3f}")
        print(f"Diversity score: {enhanced_metrics['diversity']:.3f}")
        print(f"Coverage: {enhanced_metrics['coverage']:.3f}")
        
        return accuracy

    def run_ngram_model(self, n=3):
        """è¿è¡ŒN-gramæ¨¡å‹"""
        print(f"\n--- {n}-gramæ¨¡å‹ ---")
        
        # æ„å»ºn-gramç»Ÿè®¡
        ngram_counts = defaultdict(Counter)
        
        for i in range(len(self.y_train)):
            if len(self.X_train[i]) >= n-1:
                # ä½¿ç”¨åºåˆ—çš„æœ€ån-1ä¸ªtokenä½œä¸ºä¸Šä¸‹æ–‡
                context = tuple(self.X_train[i][-(n-1):])
                next_token = self.y_train[i]
                ngram_counts[context][next_token] += 1
        
        # é¢„æµ‹
        y_pred_ngram = []
        fallback_token = Counter(self.y_train).most_common(1)[0][0]
        
        for seq in self.X_test:
            if len(seq) >= n-1:
                context = tuple(seq[-(n-1):])
                
                if context in ngram_counts and ngram_counts[context]:
                    pred = ngram_counts[context].most_common(1)[0][0]
                    y_pred_ngram.append(pred)
                else:
                    y_pred_ngram.append(fallback_token)
            else:
                y_pred_ngram.append(fallback_token)
        
        accuracy = accuracy_score(self.y_test, y_pred_ngram)
        
        # è®¡ç®—å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        enhanced_metrics = self.evaluate_enhanced_metrics(f'{n}gram', y_pred_ngram)
        
        self.results[f'{n}gram'] = {
            'accuracy': accuracy,
            'patterns': len(ngram_counts),
            **enhanced_metrics
        }
        
        print(f"å­¦ä¹ åˆ°çš„{n}-gramæ¨¡å¼æ•°: {len(ngram_counts)}")
        print(f"Top-1 å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"Top-3 å‡†ç¡®ç‡: {enhanced_metrics['top_3_accuracy']:.3f}")
        print(f"Top-5 å‡†ç¡®ç‡: {enhanced_metrics['top_5_accuracy']:.3f}")
        print(f"æ–°é¢–æ€§åˆ†æ•°: {enhanced_metrics['novelty']:.3f}")
        print(f"å¤šæ ·æ€§åˆ†æ•°: {enhanced_metrics['diversity']:.3f}")
        print(f"è¦†ç›–ç‡: {enhanced_metrics['coverage']:.3f}")
        
        return accuracy

    def run_lstm_model(self):
        """è®­ç»ƒå¹¶è¯„ä¼°LSTMæ¨¡å‹"""
        if not HAS_TENSORFLOW:
            print("\n--- è·³è¿‡LSTMæ¨¡å‹ (éœ€è¦TensorFlow) ---")
            return 0
            
        print("\n--- LSTMæ¨¡å‹ ---")
        
        vocab_size = len(self.vocab)
        seq_length = self.X_train.shape[1]
        
        # æ„å»ºæ¨¡å‹
        model = Sequential([
            Embedding(input_dim=vocab_size, output_dim=32, input_length=seq_length),
            LSTM(64, return_sequences=True, dropout=0.2),
            LSTM(32, dropout=0.2),
            Dense(vocab_size, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # è®­ç»ƒæ¨¡å‹
        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        
        history = model.fit(
            self.X_train, self.y_train,
            epochs=20,
            batch_size=min(32, len(self.X_train)//4),
            validation_split=0.2,
            callbacks=[early_stopping],
            verbose=1
        )
        
        # è¯„ä¼°æ¨¡å‹
        y_pred_proba = model.predict(self.X_test, verbose=0)
        y_pred_lstm = np.argmax(y_pred_proba, axis=1)
        
        accuracy = accuracy_score(self.y_test, y_pred_lstm)
        
        # è®¡ç®—å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        enhanced_metrics = self.evaluate_enhanced_metrics('lstm', y_pred_lstm, y_pred_proba)
        
        self.results['lstm'] = {
            'accuracy': accuracy,
            'epochs_trained': len(history.history['loss']),
            **enhanced_metrics
        }
        
        print(f"è®­ç»ƒè½®æ•°: {len(history.history['loss'])}")
        print(f"Top-1 å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"Top-3 å‡†ç¡®ç‡: {enhanced_metrics['top_3_accuracy']:.3f}")
        print(f"Top-5 å‡†ç¡®ç‡: {enhanced_metrics['top_5_accuracy']:.3f}")
        print(f"æ–°é¢–æ€§åˆ†æ•°: {enhanced_metrics['novelty']:.3f}")
        print(f"å¤šæ ·æ€§åˆ†æ•°: {enhanced_metrics['diversity']:.3f}")
        print(f"è¦†ç›–ç‡: {enhanced_metrics['coverage']:.3f}")
        
        return accuracy
    
    def run_gru_model(self):
        """è®­ç»ƒå¹¶è¯„ä¼°GRUæ¨¡å‹"""
        if not HAS_TENSORFLOW:
            print("\n--- è·³è¿‡GRUæ¨¡å‹ (éœ€è¦TensorFlow) ---")
            return 0
            
        print("\n--- GRUæ¨¡å‹ ---")
        
        vocab_size = len(self.vocab)
        seq_length = self.X_train.shape[1]
        
        # æ„å»ºGRUæ¨¡å‹
        model = Sequential([
            Embedding(input_dim=vocab_size, output_dim=32, input_length=seq_length),
            GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),
            GRU(32, dropout=0.2, recurrent_dropout=0.2),
            Dense(vocab_size, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("æ¨¡å‹æ¶æ„:")
        print(f"- è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"- åºåˆ—é•¿åº¦: {seq_length}")
        print(f"- GRUå±‚é…ç½®: 64->32 units with dropout")
        
        # è®­ç»ƒæ¨¡å‹
        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        
        history = model.fit(
            self.X_train, self.y_train,
            epochs=20,
            batch_size=min(32, len(self.X_train)//4),
            validation_split=0.2,
            callbacks=[early_stopping],
            verbose=1
        )
        
        # è¯„ä¼°æ¨¡å‹
        y_pred_proba = model.predict(self.X_test, verbose=0)
        y_pred_gru = np.argmax(y_pred_proba, axis=1)
        
        accuracy = accuracy_score(self.y_test, y_pred_gru)
        
        # è®¡ç®—å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        enhanced_metrics = self.evaluate_enhanced_metrics('gru', y_pred_gru, y_pred_proba)
        
        self.results['gru'] = {
            'accuracy': accuracy,
            'epochs_trained': len(history.history['loss']),
            'final_loss': history.history['loss'][-1],
            'final_val_loss': history.history['val_loss'][-1] if 'val_loss' in history.history else None,
            **enhanced_metrics
        }
        
        print(f"è®­ç»ƒè½®æ•°: {len(history.history['loss'])}")
        print(f"æœ€ç»ˆæŸå¤±: {history.history['loss'][-1]:.4f}")
        if 'val_loss' in history.history:
            print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {history.history['val_loss'][-1]:.4f}")
        print(f"Top-1 å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"Top-3 å‡†ç¡®ç‡: {enhanced_metrics['top_3_accuracy']:.3f}")
        print(f"Top-5 å‡†ç¡®ç‡: {enhanced_metrics['top_5_accuracy']:.3f}")
        print(f"æ–°é¢–æ€§åˆ†æ•°: {enhanced_metrics['novelty']:.3f}")
        print(f"å¤šæ ·æ€§åˆ†æ•°: {enhanced_metrics['diversity']:.3f}")
        print(f"è¦†ç›–ç‡: {enhanced_metrics['coverage']:.3f}")
        
        return accuracy
    
    def run_transformer_model_with_ablation(self, use_webfast_features=True):
        """
        è¿è¡ŒTransformeræ¨¡å‹ï¼Œæ”¯æŒæ¶ˆèç ”ç©¶
        use_webfast_features=True: ä½¿ç”¨WebFASTç‰¹å¾
        use_webfast_features=False: ä½¿ç”¨åŸºçº¿ç‰¹å¾
        """
        if not HAS_TENSORFLOW:
            feature_type = "WebFAST" if use_webfast_features else "Baseline"
            print(f"\n--- è·³è¿‡Transformeræ¨¡å‹æ¶ˆèç ”ç©¶ ({feature_type}) (éœ€è¦TensorFlow) ---")
            return 0
        
        feature_type = "WebFAST" if use_webfast_features else "Baseline"
        print(f"\n--- Transformeræ¨¡å‹æ¶ˆèç ”ç©¶ ({feature_type}ç‰¹å¾) ---")
        
        # å‡†å¤‡ç‰¹å¾æ•°æ®
        if use_webfast_features:
            # ä½¿ç”¨WebFASTç‰¹å¾é‡æ–°å¤„ç†åºåˆ—
            X_features = []
            for seq_indices in self.X_train:
                # ä»ç´¢å¼•é‡å»ºåºåˆ—å¯¹è±¡ï¼ˆæ¨¡æ‹Ÿï¼‰
                sequence = [type('Event', (), {'token': self.id_to_token.get(idx, 'unknown'), 
                                              'enhanced_token': self.id_to_token.get(idx, 'unknown'),
                                              'timestamp': i * 1000})() 
                           for i, idx in enumerate(seq_indices)]
                features = self.extract_webfast_features(sequence)
                X_features.append(features)
            
            X_test_features = []
            for seq_indices in self.X_test:
                sequence = [type('Event', (), {'token': self.id_to_token.get(idx, 'unknown'),
                                              'enhanced_token': self.id_to_token.get(idx, 'unknown'),
                                              'timestamp': i * 1000})() 
                           for i, idx in enumerate(seq_indices)]
                features = self.extract_webfast_features(sequence)
                X_test_features.append(features)
        else:
            # ä½¿ç”¨åŸºçº¿ç‰¹å¾
            X_features = []
            for seq_indices in self.X_train:
                sequence = [type('Event', (), {'token': self.id_to_token.get(idx, 'unknown'),
                                              'enhanced_token': self.id_to_token.get(idx, 'unknown'),
                                              'timestamp': i * 1000})() 
                           for i, idx in enumerate(seq_indices)]
                features = self.extract_baseline_features(sequence)
                X_features.append(features)
            
            X_test_features = []
            for seq_indices in self.X_test:
                sequence = [type('Event', (), {'token': self.id_to_token.get(idx, 'unknown'),
                                              'enhanced_token': self.id_to_token.get(idx, 'unknown'),
                                              'timestamp': i * 1000})() 
                           for i, idx in enumerate(seq_indices)]
                features = self.extract_baseline_features(sequence)
                X_test_features.append(features)
        
        X_features = np.array(X_features)
        X_test_features = np.array(X_test_features)
        
        vocab_size = len(self.vocab)
        feature_dim = X_features.shape[1]
        embed_dim = 64
        num_heads = 4
        ff_dim = 128
        
        print("æ¨¡å‹æ¶æ„:")
        print(f"- è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"- ç‰¹å¾ç»´åº¦: {feature_dim}")
        print(f"- åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"- æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
        print(f"- å‰é¦ˆç»´åº¦: {ff_dim}")
        print(f"- ç‰¹å¾ç±»å‹: {feature_type}")
        
        # æ„å»ºé€‚åº”ç‰¹å¾è¾“å…¥çš„Transformeræ¨¡å‹
        inputs = Input(shape=(feature_dim,))
        
        # ç‰¹å¾åµŒå…¥å±‚
        embedding_layer = Dense(embed_dim, activation='relu')(inputs)
        embedding_layer = tf.expand_dims(embedding_layer, axis=1)  # æ·»åŠ åºåˆ—ç»´åº¦
        
        # Transformer block
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )(embedding_layer, embedding_layer)
        
        # Add & Norm
        attention_output = LayerNormalization(epsilon=1e-6)(embedding_layer + attention_output)
        
        # Feed Forward
        ffn_output = Dense(ff_dim, activation="relu")(attention_output)
        ffn_output = Dense(embed_dim)(ffn_output)
        
        # Add & Norm  
        ffn_output = LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)
        
        # Global average pooling
        sequence_output = GlobalAveragePooling1D()(ffn_output)
        
        # Dropout and classification
        sequence_output = Dropout(0.3)(sequence_output)
        outputs = Dense(vocab_size, activation="softmax")(sequence_output)
        
        model = Model(inputs=inputs, outputs=outputs)
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss="sparse_categorical_crossentropy",
            metrics=["accuracy"]
        )
        
        # è®­ç»ƒæ¨¡å‹
        early_stopping = EarlyStopping(
            monitor='val_loss', 
            patience=5, 
            restore_best_weights=True,
            min_delta=0.001
        )
        
        history = model.fit(
            X_features, self.y_train,
            epochs=30,
            batch_size=min(16, len(X_features)//4),
            validation_split=0.2,
            callbacks=[early_stopping],
            verbose=1
        )
        
        # è¯„ä¼°æ¨¡å‹
        y_pred_proba = model.predict(X_test_features, verbose=0)
        y_pred_transformer = np.argmax(y_pred_proba, axis=1)
        
        accuracy = accuracy_score(self.y_test, y_pred_transformer)
        
        # è®¡ç®—å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        enhanced_metrics = self.evaluate_enhanced_metrics(f'transformer_{feature_type.lower()}', y_pred_transformer, y_pred_proba)
        
        # å­˜å‚¨æ¶ˆèç ”ç©¶ç»“æœ
        model_key = f'transformer_{feature_type.lower()}'
        self.ablation_results[model_key] = {
            'accuracy': accuracy,
            'feature_type': feature_type,
            'epochs_trained': len(history.history['loss']),
            'final_loss': history.history['loss'][-1],
            'final_val_loss': history.history['val_loss'][-1] if 'val_loss' in history.history else None,
            'num_heads': num_heads,
            'embed_dim': embed_dim,
            'ff_dim': ff_dim,
            **enhanced_metrics
        }
        
        print(f"è®­ç»ƒè½®æ•°: {len(history.history['loss'])}")
        print(f"æœ€ç»ˆæŸå¤±: {history.history['loss'][-1]:.4f}")
        if 'val_loss' in history.history:
            print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {history.history['val_loss'][-1]:.4f}")
        print(f"Top-1 å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"Top-3 å‡†ç¡®ç‡: {enhanced_metrics['top_3_accuracy']:.3f}")
        print(f"Top-5 å‡†ç¡®ç‡: {enhanced_metrics['top_5_accuracy']:.3f}")
        print(f"æ–°é¢–æ€§åˆ†æ•°: {enhanced_metrics['novelty']:.3f}")
        print(f"å¤šæ ·æ€§åˆ†æ•°: {enhanced_metrics['diversity']:.3f}")
        print(f"è¦†ç›–ç‡: {enhanced_metrics['coverage']:.3f}")
        
        return accuracy
    
    def run_transformer_model(self):
        """è®­ç»ƒå¹¶è¯„ä¼°Transformeræ¨¡å‹"""
        if not HAS_TENSORFLOW:
            print("\n--- è·³è¿‡Transformeræ¨¡å‹ (éœ€è¦TensorFlow) ---")
            return 0
            
        print("\n--- Transformeræ¨¡å‹ ---")
        
        vocab_size = len(self.vocab)
        seq_length = self.X_train.shape[1]
        embed_dim = 64
        num_heads = 4
        ff_dim = 128
        
        print("æ¨¡å‹æ¶æ„:")
        print(f"- è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"- åºåˆ—é•¿åº¦: {seq_length}")
        print(f"- åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"- æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
        print(f"- å‰é¦ˆç»´åº¦: {ff_dim}")
        
        # æ„å»ºTransformeræ¨¡å‹
        inputs = Input(shape=(seq_length,))
        
        # Embedding layer
        embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)
        
        # Transformer block
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )(embedding_layer, embedding_layer)
        
        # Add & Norm
        attention_output = LayerNormalization(epsilon=1e-6)(embedding_layer + attention_output)
        
        # Feed Forward
        ffn_output = Dense(ff_dim, activation="relu")(attention_output)
        ffn_output = Dense(embed_dim)(ffn_output)
        
        # Add & Norm  
        ffn_output = LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)
        
        # Global average pooling
        sequence_output = GlobalAveragePooling1D()(ffn_output)
        
        # Dropout and classification
        sequence_output = Dropout(0.3)(sequence_output)
        outputs = Dense(vocab_size, activation="softmax")(sequence_output)
        
        model = Model(inputs=inputs, outputs=outputs)
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss="sparse_categorical_crossentropy",
            metrics=["accuracy"]
        )
        
        # è®­ç»ƒæ¨¡å‹
        early_stopping = EarlyStopping(
            monitor='val_loss', 
            patience=5, 
            restore_best_weights=True,
            min_delta=0.001
        )
        
        history = model.fit(
            self.X_train, self.y_train,
            epochs=30,
            batch_size=min(16, len(self.X_train)//4),  # Smaller batch size for Transformer
            validation_split=0.2,
            callbacks=[early_stopping],
            verbose=1
        )
        
        # è¯„ä¼°æ¨¡å‹
        y_pred_proba = model.predict(self.X_test, verbose=0)
        y_pred_transformer = np.argmax(y_pred_proba, axis=1)
        
        accuracy = accuracy_score(self.y_test, y_pred_transformer)
        
        # è®¡ç®—å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        enhanced_metrics = self.evaluate_enhanced_metrics('transformer', y_pred_transformer, y_pred_proba)
        
        self.results['transformer'] = {
            'accuracy': accuracy,
            'epochs_trained': len(history.history['loss']),
            'final_loss': history.history['loss'][-1],
            'final_val_loss': history.history['val_loss'][-1] if 'val_loss' in history.history else None,
            'num_heads': num_heads,
            'embed_dim': embed_dim,
            'ff_dim': ff_dim,
            **enhanced_metrics
        }
        
        print(f"è®­ç»ƒè½®æ•°: {len(history.history['loss'])}")
        print(f"æœ€ç»ˆæŸå¤±: {history.history['loss'][-1]:.4f}")
        if 'val_loss' in history.history:
            print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {history.history['val_loss'][-1]:.4f}")
        print(f"Top-1 å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"Top-3 å‡†ç¡®ç‡: {enhanced_metrics['top_3_accuracy']:.3f}")
        print(f"Top-5 å‡†ç¡®ç‡: {enhanced_metrics['top_5_accuracy']:.3f}")
        print(f"æ–°é¢–æ€§åˆ†æ•°: {enhanced_metrics['novelty']:.3f}")
        print(f"å¤šæ ·æ€§åˆ†æ•°: {enhanced_metrics['diversity']:.3f}")
        print(f"è¦†ç›–ç‡: {enhanced_metrics['coverage']:.3f}")
        
        return accuracy

    def analyze_prediction_patterns(self):
        """åˆ†æé¢„æµ‹æ¨¡å¼å’Œé”™è¯¯"""
        print("\n--- é¢„æµ‹æ¨¡å¼åˆ†æ ---")
        
        # åˆ†ææœ€å¸¸è§çš„åºåˆ—æ¨¡å¼
        sequence_patterns = Counter()
        for seq in self.X_train:
            pattern = tuple(seq)
            sequence_patterns[pattern] += 1
        
        print(f"æ€»åºåˆ—æ¨¡å¼æ•°: {len(sequence_patterns)}")
        print(f"æœ€å¸¸è§çš„5ä¸ªåºåˆ—æ¨¡å¼:")
        
        for pattern, count in sequence_patterns.most_common(5):
            token_names = [self.id_to_token[id] for id in pattern]
            print(f"  {' -> '.join(token_names)}: {count}æ¬¡")
        
        # åˆ†ætokenåˆ†å¸ƒ
        token_dist = Counter([self.id_to_token[id] for id in self.y_train])
        print(f"\næœ€å¸¸è§çš„5ä¸ªä¸‹ä¸€åŠ¨ä½œ:")
        for token, count in token_dist.most_common(5):
            print(f"  {token}: {count}æ¬¡ ({count/len(self.y_train):.1%})")

    def run_ablation_study(self):
        """
        è¿è¡Œæ¶ˆèç ”ç©¶ï¼šæ¯”è¾ƒWebFASTç‰¹å¾ vs åŸºçº¿ç‰¹å¾çš„Transformeræ¨¡å‹æ€§èƒ½
        """
        print("\n" + "="*60)
        print("æ¶ˆèç ”ç©¶: WebFASTç‰¹å¾ vs åŸºçº¿ç‰¹å¾")
        print("="*60)
        
        # è¿è¡Œå¸¦WebFASTç‰¹å¾çš„Transformer
        webfast_acc = self.run_transformer_model_with_ablation(use_webfast_features=True)
        
        # è¿è¡Œå¸¦åŸºçº¿ç‰¹å¾çš„Transformer
        baseline_acc = self.run_transformer_model_with_ablation(use_webfast_features=False)
        
        # åˆ†æç»“æœ
        self.analyze_ablation_results()
    
    def analyze_ablation_results(self):
        """
        åˆ†ææ¶ˆèç ”ç©¶ç»“æœ
        """
        print("\n" + "="*50)
        print("æ¶ˆèç ”ç©¶ç»“æœåˆ†æ")
        print("="*50)
        
        if 'transformer_webfast' not in self.ablation_results or 'transformer_baseline' not in self.ablation_results:
            print("æ¶ˆèç ”ç©¶æ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return
        
        webfast_results = self.ablation_results['transformer_webfast']
        baseline_results = self.ablation_results['transformer_baseline']
        
        webfast_acc = webfast_results['accuracy']
        baseline_acc = baseline_results['accuracy']
        
        print(f"\nğŸ“Š å‡†ç¡®ç‡å¯¹æ¯”:")
        print(f"  Transformer (WebFASTç‰¹å¾):    {webfast_acc:.4f}")
        print(f"  Transformer (åŸºçº¿ç‰¹å¾):       {baseline_acc:.4f}")
        
        if baseline_acc > 0:
            improvement = (webfast_acc - baseline_acc) / baseline_acc * 100
            print(f"  WebFASTç›¸å¯¹æå‡:             {improvement:+.1f}%")
        
        print(f"\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”:")
        metrics_to_compare = ['top_3_accuracy', 'top_5_accuracy', 'novelty', 'diversity', 'coverage']
        
        for metric in metrics_to_compare:
            webfast_val = webfast_results.get(metric, 0)
            baseline_val = baseline_results.get(metric, 0)
            
            if baseline_val > 0:
                improvement = (webfast_val - baseline_val) / baseline_val * 100
                print(f"  {metric}: WebFAST={webfast_val:.3f}, åŸºçº¿={baseline_val:.3f}, æå‡={improvement:+.1f}%")
            else:
                print(f"  {metric}: WebFAST={webfast_val:.3f}, åŸºçº¿={baseline_val:.3f}")
        
        # è®­ç»ƒæ•ˆç‡å¯¹æ¯”
        print(f"\nâ±ï¸ è®­ç»ƒæ•ˆç‡å¯¹æ¯”:")
        print(f"  WebFASTè®­ç»ƒè½®æ•°: {webfast_results.get('epochs_trained', 0)}")
        print(f"  åŸºçº¿è®­ç»ƒè½®æ•°:    {baseline_results.get('epochs_trained', 0)}")
        print(f"  WebFASTæœ€ç»ˆæŸå¤±: {webfast_results.get('final_loss', 0):.4f}")
        print(f"  åŸºçº¿æœ€ç»ˆæŸå¤±:    {baseline_results.get('final_loss', 0):.4f}")
        
        # ç»“è®º
        print(f"\nğŸ¯ æ¶ˆèç ”ç©¶ç»“è®º:")
        if webfast_acc > baseline_acc * 1.05:  # 5%ä»¥ä¸Šçš„æå‡
            print(f"  âœ… WebFASTç‰¹å¾æ˜¾è‘—ä¼˜äºåŸºçº¿ç‰¹å¾")
            print(f"  âœ… DCTå˜æ¢æœ‰æ•ˆæ•æ‰äº†æ—¶åºæ¨¡å¼")
            print(f"  âœ… æ€§èƒ½æå‡ä¸»è¦æ¥æºäºWebFASTè¡¨ç¤ºæ³•")
        elif webfast_acc > baseline_acc:
            print(f"  âœ… WebFASTç‰¹å¾ç•¥ä¼˜äºåŸºçº¿ç‰¹å¾")
            print(f"  âš ï¸  æå‡å¹…åº¦è¾ƒå°ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®éªŒè¯")
        else:
            print(f"  âŒ WebFASTç‰¹å¾æœªæ˜¾ç¤ºæ˜æ˜¾ä¼˜åŠ¿")
            print(f"  âš ï¸  å¯èƒ½éœ€è¦è°ƒä¼˜ç‰¹å¾æå–æˆ–æ¨¡å‹æ¶æ„")
    
    def visualize_ablation_results(self):
        """
        å¯è§†åŒ–æ¶ˆèç ”ç©¶ç»“æœ
        """
        if not self.ablation_results:
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        models = ['WebFASTç‰¹å¾', 'åŸºçº¿ç‰¹å¾']
        webfast_acc = self.ablation_results.get('transformer_webfast', {}).get('accuracy', 0)
        baseline_acc = self.ablation_results.get('transformer_baseline', {}).get('accuracy', 0)
        accuracies = [webfast_acc, baseline_acc]
        
        colors = ['#4CAF50', '#FF9800']  # ç»¿è‰²ä¸ºWebFASTï¼Œæ©™è‰²ä¸ºåŸºçº¿
        bars1 = axes[0].bar(models, accuracies, color=colors, alpha=0.8)
        axes[0].set_title('æ¶ˆèç ”ç©¶: Transformeræ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        axes[0].set_ylabel('å‡†ç¡®ç‡')
        axes[0].set_ylim(0, max(accuracies) * 1.2)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars1, accuracies):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(accuracies)*0.01,
                        f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
        
        axes[0].grid(True, alpha=0.3, axis='y')
        
        # å¤šæŒ‡æ ‡é›·è¾¾å›¾å¯¹æ¯”
        if len(self.ablation_results) >= 2:
            metrics = ['accuracy', 'top_3_accuracy', 'diversity', 'coverage']
            metric_labels = ['Top-1å‡†ç¡®ç‡', 'Top-3å‡†ç¡®ç‡', 'å¤šæ ·æ€§', 'è¦†ç›–ç‡']
            
            webfast_values = [self.ablation_results.get('transformer_webfast', {}).get(m, 0) for m in metrics]
            baseline_values = [self.ablation_results.get('transformer_baseline', {}).get(m, 0) for m in metrics]
            
            x_pos = np.arange(len(metric_labels))
            width = 0.35
            
            axes[1].bar(x_pos - width/2, webfast_values, width, label='WebFASTç‰¹å¾', color='#4CAF50', alpha=0.8)
            axes[1].bar(x_pos + width/2, baseline_values, width, label='åŸºçº¿ç‰¹å¾', color='#FF9800', alpha=0.8)
            
            axes[1].set_title('å¤šæŒ‡æ ‡æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
            axes[1].set_ylabel('å¾—åˆ†')
            axes[1].set_xticks(x_pos)
            axes[1].set_xticklabels(metric_labels, rotation=45, ha='right')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_file = 'experiment_2_ablation_study.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"\næ¶ˆèç ”ç©¶ç»“æœå›¾è¡¨å·²ä¿å­˜è‡³ {output_file}")
        plt.show()
    
    def visualize_results(self):
        """Visualize experiment results"""
        if not self.results:
            print("No results to visualize")
            return
        
        # Prepare data
        models = []
        accuracies = []
        
        for model_name, metrics in self.results.items():
            models.append(model_name.upper())
            accuracies.append(metrics['accuracy'])
        
        # Create enhanced charts
        plt.figure(figsize=(16, 12))
        
        # Chart A: Model accuracy comparison (Top-1, Top-3, Top-5)
        plt.subplot(2, 3, 1)
        x_pos = np.arange(len(models))
        width = 0.25
        
        top1_scores = [metrics['accuracy'] for metrics in self.results.values()]
        top3_scores = [metrics.get('top_3_accuracy', 0) for metrics in self.results.values()]
        top5_scores = [metrics.get('top_5_accuracy', 0) for metrics in self.results.values()]
        
        plt.bar(x_pos - width, top1_scores, width, label='Top-1', color='skyblue')
        plt.bar(x_pos, top3_scores, width, label='Top-3', color='lightcoral')  
        plt.bar(x_pos + width, top5_scores, width, label='Top-5', color='lightgreen')
        
        plt.title('(A) Top-K Accuracy Comparison')
        plt.ylabel('Accuracy')
        plt.xlabel('Model')
        plt.xticks(x_pos, models, rotation=45)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Chart B: Novelty & Diversity Comparison  
        plt.subplot(2, 3, 2)
        novelty_scores = [metrics.get('novelty', 0) for metrics in self.results.values()]
        diversity_scores = [metrics.get('diversity', 0) for metrics in self.results.values()]
        
        x_pos = np.arange(len(models))
        width = 0.35
        plt.bar(x_pos - width/2, novelty_scores, width, label='Novelty', color='orange')
        plt.bar(x_pos + width/2, diversity_scores, width, label='Diversity', color='purple')
        
        plt.title('(B) Novelty & Diversity Scores')
        plt.ylabel('Score')
        plt.xlabel('Model')
        plt.xticks(x_pos, models, rotation=45)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Chart C: Coverage Analysis
        plt.subplot(2, 3, 3)
        coverage_scores = [metrics.get('coverage', 0) for metrics in self.results.values()]
        bars = plt.bar(models, coverage_scores, color='teal')
        plt.title('(C) Prediction Coverage')
        plt.ylabel('Coverage Rate')
        plt.xlabel('Model')
        plt.xticks(rotation=45)
        
        # Add value labels
        for bar, cov in zip(bars, coverage_scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{cov:.2f}', ha='center', va='bottom')
        plt.grid(True, alpha=0.3)
        
        # Chart D: Action Frequency Distribution
        plt.subplot(2, 3, 4)
        token_dist = Counter([self.id_to_token[id] for id in self.y_train])
        top_tokens = dict(token_dist.most_common(8))
        plt.bar(range(len(top_tokens)), list(top_tokens.values()), color='lightblue')
        plt.title('(D) Top-8 Action Frequency')
        plt.ylabel('Frequency')
        plt.xlabel('Action Type')
        plt.xticks(range(len(top_tokens)), list(top_tokens.keys()), rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Chart E: Prediction Entropy
        plt.subplot(2, 3, 5)
        entropy_scores = [metrics.get('prediction_entropy', 0) for metrics in self.results.values()]
        bars = plt.bar(models, entropy_scores, color='salmon')
        plt.title('(E) Prediction Entropy')
        plt.ylabel('Entropy (bits)')
        plt.xlabel('Model')
        plt.xticks(rotation=45)
        
        # Add value labels  
        for bar, ent in zip(bars, entropy_scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{ent:.2f}', ha='center', va='bottom')
        plt.grid(True, alpha=0.3)
        
        # Chart F: æ¶ˆèç ”ç©¶ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        plt.subplot(2, 3, 6)
        if self.ablation_results and len(self.ablation_results) >= 2:
            # æ˜¾ç¤ºæ¶ˆèç ”ç©¶ç»“æœ
            webfast_acc = self.ablation_results.get('transformer_webfast', {}).get('accuracy', 0)
            baseline_acc = self.ablation_results.get('transformer_baseline', {}).get('accuracy', 0)
            
            models = ['WebFAST\nç‰¹å¾', 'åŸºçº¿\nç‰¹å¾']
            accuracies = [webfast_acc, baseline_acc]
            colors = ['#4CAF50', '#FF9800']
            
            bars = plt.bar(models, accuracies, color=colors, alpha=0.8)
            plt.title('(F) æ¶ˆèç ”ç©¶: ç‰¹å¾å¯¹æ¯”', fontweight='bold')
            plt.ylabel('å‡†ç¡®ç‡')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œæå‡ç™¾åˆ†æ¯”
            for bar, acc in zip(bars, accuracies):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(accuracies)*0.01,
                        f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
            
            if baseline_acc > 0:
                improvement = (webfast_acc - baseline_acc) / baseline_acc * 100
                plt.text(0.5, 0.8, f'æå‡: {improvement:+.1f}%', 
                        transform=plt.gca().transAxes, ha='center',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
            
            plt.grid(True, alpha=0.3)
        elif self.results:
            # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹çš„ç»¼åˆæŒ‡æ ‡ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            best_model = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
            best_metrics = self.results[best_model]
            
            metric_names = ['Top-1', 'Top-3', 'Novelty', 'Diversity', 'Coverage']
            metric_values = [
                best_metrics.get('accuracy', 0),
                best_metrics.get('top_3_accuracy', 0),
                best_metrics.get('novelty', 0) / 10,  # æ ‡å‡†åŒ–åˆ°[0,1]
                best_metrics.get('diversity', 0),
                best_metrics.get('coverage', 0)
            ]
            
            bars = plt.bar(metric_names, metric_values, color=['gold', 'orange', 'red', 'purple', 'teal'])
            plt.title(f'(F) Best Model ({best_model.upper()}) Metrics')
            plt.ylabel('Normalized Score')
            plt.xticks(rotation=45)
            
            # Add value labels
            for bar, val in zip(bars, metric_values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{val:.2f}', ha='center', va='bottom')
            plt.grid(True, alpha=0.3)
        else:
            plt.text(0.5, 0.5, 'No Results Available', ha='center', va='center', 
                    transform=plt.gca().transAxes)
            plt.title('(F) Model Performance Summary')
        
        plt.tight_layout()
        output_file = 'experiment_2_prediction_results.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"\nResults chart saved to {output_file}")
        plt.show()

    def generate_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„å®éªŒæŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ä¸‹ä¸€åŠ¨ä½œé¢„æµ‹å‡†ç¡®ç‡å®éªŒæŠ¥å‘Š")
        print("="*60)
        
        # æ•°æ®é›†ä¿¡æ¯
        print(f"\næ•°æ®é›†ä¿¡æ¯:")
        print(f"- æ€»äº‹ä»¶æ•°: {len(self.df)}")
        print(f"- å”¯ä¸€åŠ¨ä½œç±»å‹æ•°: {len(self.vocab)}")
        print(f"- è®­ç»ƒåºåˆ—æ•°: {len(self.X_train)}")
        print(f"- æµ‹è¯•åºåˆ—æ•°: {len(self.X_test)}")
        
        # å¢å¼ºçš„æ¨¡å‹æ€§èƒ½å¯¹æ¯”
        print(f"\nå¢å¼ºè¯„ä¼°æŒ‡æ ‡å¯¹æ¯”:")
        if self.results:
            best_model = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
            best_accuracy = self.results[best_model]['accuracy']
            
            print(f"{'æ¨¡å‹':<10} {'Top-1':<8} {'Top-3':<8} {'Top-5':<8} {'æ–°é¢–æ€§':<8} {'å¤šæ ·æ€§':<8} {'è¦†ç›–ç‡':<8}")
            print("-" * 60)
            
            for model_name, metrics in sorted(self.results.items(), 
                                            key=lambda x: x[1]['accuracy'], reverse=True):
                top1 = metrics.get('accuracy', 0)
                top3 = metrics.get('top_3_accuracy', 0)  
                top5 = metrics.get('top_5_accuracy', 0)
                novelty = metrics.get('novelty', 0)
                diversity = metrics.get('diversity', 0)
                coverage = metrics.get('coverage', 0)
                
                print(f"{model_name.upper():<10} {top1:<8.3f} {top3:<8.3f} {top5:<8.3f} "
                      f"{novelty:<8.2f} {diversity:<8.3f} {coverage:<8.3f}")
            
            print(f"\næœ€ä½³æ¨¡å‹: {best_model.upper()} (Top-1å‡†ç¡®ç‡: {best_accuracy:.3f})")
            
            # é¢å¤–çš„æ´å¯Ÿåˆ†æ
            print(f"\nå…³é”®æ´å¯Ÿ:")
            baseline_acc = self.results.get('frequency', {}).get('accuracy', 0)
            if baseline_acc > 0:
                improvement = (best_accuracy / baseline_acc - 1) * 100
                print(f"- æœ€ä½³æ¨¡å‹ç›¸å¯¹åŸºçº¿æå‡: {improvement:.1f}%")
            
            # åˆ†æTop-Kå‡†ç¡®ç‡çš„æå‡æ•ˆæœ
            if best_model in self.results:
                best_metrics = self.results[best_model]
                top1_acc = best_metrics.get('accuracy', 0)
                top3_acc = best_metrics.get('top_3_accuracy', 0)
                top5_acc = best_metrics.get('top_5_accuracy', 0)
                
                if top3_acc > top1_acc:
                    top3_improvement = (top3_acc / top1_acc - 1) * 100
                    print(f"- Top-3ç›¸å¯¹Top-1æå‡: {top3_improvement:.1f}%")
                if top5_acc > top1_acc:
                    top5_improvement = (top5_acc / top1_acc - 1) * 100  
                    print(f"- Top-5ç›¸å¯¹Top-1æå‡: {top5_improvement:.1f}%")
                    
                # è¯„ä¼°æ–°é¢–æ€§å’Œå¤šæ ·æ€§
                novelty = best_metrics.get('novelty', 0)
                diversity = best_metrics.get('diversity', 0)
                print(f"- æ¨¡å‹æ–°é¢–æ€§è¯„åˆ†: {novelty:.2f} (é«˜åˆ†è¡¨ç¤ºèƒ½é¢„æµ‹ç½•è§åŠ¨ä½œ)")
                print(f"- æ¨¡å‹å¤šæ ·æ€§è¯„åˆ†: {diversity:.3f} (é«˜åˆ†è¡¨ç¤ºé¢„æµ‹ç»“æœå¤šæ ·)")
                
                coverage = best_metrics.get('coverage', 0)  
                print(f"- åŠ¨ä½œè¦†ç›–ç‡: {coverage:.1%} (è¦†ç›– {int(coverage * len(self.vocab))} / {len(self.vocab)} ç§åŠ¨ä½œ)")
        
        print(f"\nè¿™äº›å¢å¼ºæŒ‡æ ‡çš„æ„ä¹‰:")
        print(f"- Top-Kå‡†ç¡®ç‡ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œåªè¦çœŸå®åŠ¨ä½œåœ¨å‰Kä¸ªé¢„æµ‹ä¸­å³å¯è¢«è®¤ä¸ºæ˜¯æˆåŠŸçš„")
        print(f"- æ–°é¢–æ€§ï¼šé«˜åˆ†è¯´æ˜æ¨¡å‹ä¸åªæ˜¯ç®€å•é‡å¤å¸¸è§åŠ¨ä½œï¼Œèƒ½å‘ç°é•¿å°¾æ¨¡å¼")  
        print(f"- å¤šæ ·æ€§ï¼šé«˜åˆ†è¯´æ˜æ¨¡å‹é¢„æµ‹ç»“æœä¸°å¯Œï¼Œä¸ä¼šé™·å…¥å•ä¸€é¢„æµ‹æ¨¡å¼")
        print(f"- è¦†ç›–ç‡ï¼šé«˜åˆ†è¯´æ˜æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¹¿æ³›çš„åœºæ™¯ï¼Œé€‚ç”¨æ€§æ›´å¼º")
        
        # åˆ†æå’Œå»ºè®®
        print(f"\nåˆ†æä¸å»ºè®®:")
        if self.results:
            baseline_acc = self.results.get('frequency', {}).get('accuracy', 0)
            if baseline_acc > 0.5:
                print(f"- ç”¨æˆ·è¡Œä¸ºå…·æœ‰è¾ƒå¼ºçš„å¯é¢„æµ‹æ€§ (åŸºçº¿å‡†ç¡®ç‡: {baseline_acc:.1%})")
            else:
                print(f"- ç”¨æˆ·è¡Œä¸ºç›¸å¯¹éšæœº (åŸºçº¿å‡†ç¡®ç‡: {baseline_acc:.1%})")
                
            # æ·±åº¦å­¦ä¹ æ¨¡å‹æ¯”è¾ƒåˆ†æ
            deep_models = {k: v for k, v in self.results.items() if k in ['lstm', 'gru', 'transformer']}
            
            if len(deep_models) >= 2:
                print(f"\næ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
                accuracies = {model: metrics['accuracy'] for model, metrics in deep_models.items()}
                sorted_models = sorted(accuracies.items(), key=lambda x: x[1], reverse=True)
                
                for i, (model, acc) in enumerate(sorted_models):
                    print(f"  {i+1}. {model.upper()}: {acc:.3f}")
                
                best_model, best_acc = sorted_models[0]
                if len(sorted_models) > 1:
                    second_model, second_acc = sorted_models[1]
                    if second_acc > 0:
                        improvement = (best_acc / second_acc - 1) * 100
                        print(f"  â†’ {best_model.upper()}æ€§èƒ½æœ€ä½³ï¼Œæ¯”{second_model.upper()}é«˜{improvement:.1f}%")
                    else:
                        print(f"  â†’ {best_model.upper()}æ€§èƒ½æœ€ä½³ ({best_acc:.3f})ï¼Œ{second_model.upper()}æœªèƒ½è®­ç»ƒæˆåŠŸ")
                    
                if best_acc > baseline_acc * 1.1:
                    print(f"- æ·±åº¦å­¦ä¹ æ¨¡å‹({best_model.upper()})æ˜¾ç¤ºå‡ºä¼˜åŠ¿ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
                else:
                    print(f"- æ·±åº¦å­¦ä¹ æ¨¡å‹æå‡æœ‰é™ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–ç‰¹å¾å·¥ç¨‹")
                    
                # Transformerç‰¹æ®Šåˆ†æ
                if 'transformer' in deep_models:
                    transformer_acc = deep_models['transformer']['accuracy']
                    print(f"- Transformeræ¨¡å‹è¡¨ç°: ", end="")
                    if transformer_acc == best_acc:
                        print(f"æœ€ä½³ï¼Œæ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆæ•è·åºåˆ—æ¨¡å¼")
                    elif transformer_acc >= np.mean([acc for acc in accuracies.values()]):
                        print(f"è‰¯å¥½ï¼Œé€‚åˆé•¿åºåˆ—ä¾èµ–å»ºæ¨¡")
                    else:
                        print(f"ä¸€èˆ¬ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–è°ƒå‚ä¼˜åŒ–")
            
            elif len(deep_models) == 1:
                model_name, metrics = list(deep_models.items())[0]
                model_acc = metrics['accuracy']
                if model_acc > baseline_acc * 1.1:
                    print(f"- {model_name.upper()}æ¨¡å‹æ˜¾ç¤ºå‡ºä¼˜åŠ¿ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
                else:
                    print(f"- {model_name.upper()}æ¨¡å‹æå‡æœ‰é™ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–ç‰¹å¾å·¥ç¨‹")

def main():
    parser = argparse.ArgumentParser(description='å®éªŒäºŒ: ä¸‹ä¸€åŠ¨ä½œé¢„æµ‹')
    parser.add_argument('input_file', help='æ¸…æ´—åçš„CSVæ•°æ®æ–‡ä»¶')
    parser.add_argument('--skip-lstm', action='store_true', help='è·³è¿‡LSTMæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--skip-gru', action='store_true', help='è·³è¿‡GRUæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--skip-transformer', action='store_true', help='è·³è¿‡Transformeræ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--skip-deep-learning', action='store_true', help='è·³è¿‡æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--ngram', type=int, default=3, help='N-gramæ¨¡å‹çš„Nå€¼ (é»˜è®¤: 3)')
    args = parser.parse_args()
    
    if not os.path.exists(args.input_file):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {args.input_file}")
        return
    
    # æ£€æŸ¥æ•°æ®é‡
    df_check = pd.read_csv(args.input_file)
    if len(df_check) < 20:
        print("é”™è¯¯ï¼šæ•°æ®é‡è¿‡å°ï¼Œæ— æ³•è¿›è¡Œæœ‰æ„ä¹‰çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚è¯·æ”¶é›†æ›´å¤šæ•°æ®ã€‚")
        print(f"å½“å‰æ•°æ®é‡: {len(df_check)} è¡Œï¼Œå»ºè®®è‡³å°‘50è¡Œä»¥ä¸Š")
        return
    
    # è¿è¡Œå®éªŒ
    exp = PredictionExperiment(args.input_file)
    
    if not hasattr(exp, 'X_train'):
        print("é”™è¯¯ï¼šæ•°æ®å‡†å¤‡å¤±è´¥")
        return
    
    # è¿è¡Œå„ç§æ¨¡å‹
    exp.run_frequency_baseline()
    exp.run_markov_baseline()
    exp.run_ngram_model(args.ngram)
    
    # è¿è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹
    if not args.skip_deep_learning:
        if not args.skip_lstm:
            exp.run_lstm_model()
        if not args.skip_gru:
            exp.run_gru_model()
        if not args.skip_transformer:
            exp.run_transformer_model()
        
        # è¿è¡Œæ¶ˆèç ”ç©¶
        print("\n" + "="*60)
        print("å¼€å§‹æ¶ˆèç ”ç©¶...")
        print("="*60)
        exp.run_ablation_study()
    
    # åˆ†æå’Œå¯è§†åŒ–
    exp.analyze_prediction_patterns()
    exp.visualize_results()
    
    # å¯è§†åŒ–æ¶ˆèç ”ç©¶ç»“æœ
    if exp.ablation_results:
        exp.visualize_ablation_results()
    
    exp.generate_report()

if __name__ == "__main__":
    main()